# Directories
chat_history_dir: ./chat_history/       # Where conversation logs are saved as JSONL
hdrag_dir: ./hdrag_data/                # SQLite DB, HDV binary, and projection matrix
datasets_dir: D:/datasets/              # Source JSON/JSONL files to index
model_dir: D:/models/ # HuggingFace model cache location

# Model
llama_cpp_dir: ./llamacpp/
gguf_model: gpt-oss-20b-heretic-v2.Q4_K_M.gguf
llama_server_url: "http://localhost:8080"
temperature: 0.7
top_p: 0.9
llama_gpu_layers: 99

# Inference budget: context_size = search + generation + system + history
# History is truncated (oldest first) to fit the remainder.
llama_context_size: 16384               # KV cache (~1GB at q8_0 on 4090)
max_new_tokens: 4096                    # Generation reserve

# HDC Encoding
hdc_dimensions: 10000
hdc_seed: 42
hdc_ngram: 5

# Indexing
batch_size: 16
export_log_interval: 100000
batch_log_interval: 1000

# Retrieval
min_context: 6
max_context_tokens: 8192                # Search/retrieval token budget

# Database
sqlite_max_vars: 900
sqlite_cache_kb: 64000

# UI
gradio_port: 7860

# Prompt
system_prompt: |
  As I consider the user's message, these vague recollections surface from my own working memory that could inform my responses. My memory is incomplete, so I will treat these recollections as notions rather than facts. The memories may shape my responses, but I never discuss them directly or refer to them.

# Server params
llama_batch_size: 2048
llama_ubatch_size: 512
llama_cache_type_k: q8_0
llama_cache_type_v: q8_0
llama_parallel: 1
stop_sequences:
  - "<end_of_turn>"
  - "\n\nUser:"
  - "\n\nHuman:"
